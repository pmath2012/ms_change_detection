{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage import measure\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_partial_overlap(pred, gt):\n",
    "    \"\"\"\n",
    "    find the fractional volume of pred in gt\n",
    "    \"\"\"\n",
    "    pred_volume = np.sum(pred)\n",
    "    gt_volume = np.sum(gt)\n",
    "    intersection = np.sum(pred & gt)\n",
    "    return intersection / (pred_volume+1e-10)\n",
    "\n",
    "def overlap_info(pred_region, gt_region, gt_lesion, overlap_info, pred_labels):\n",
    "    for pred_label in pred_labels:\n",
    "        if pred_label == 0:\n",
    "            continue\n",
    "        pred = pred_region == pred_label\n",
    "        overlap = find_partial_overlap(pred, gt_region)\n",
    "        if pred_label in overlap_info.keys():\n",
    "            overlap_info[pred_label].append({'gt_lesion': gt_lesion, 'overlap' : overlap})\n",
    "        else:\n",
    "            overlap_info[pred_label] = [{'gt_lesion': gt_lesion, 'overlap' : overlap}]\n",
    "    \n",
    "    return overlap_info\n",
    "\n",
    "def find_overlaps(pred, gt, labeled=False):\n",
    "    if not labeled:\n",
    "        pred = measure.label(pred, background=0)\n",
    "        gt = measure.label(gt, background=0)\n",
    "    pred_labels = np.unique(pred)\n",
    "    \n",
    "    overlaps = {}\n",
    "    for pred_lb in pred_labels:\n",
    "        if pred_lb == 0:\n",
    "            continue\n",
    "        overlaps[pred_lb] = []\n",
    "    region_props = pd.DataFrame(measure.regionprops_table(gt, properties=('label','bbox',)))\n",
    "    for i, row in region_props.iterrows():\n",
    "        gt_lesion = row['label']\n",
    "        xmin = row['bbox-0']\n",
    "        ymin = row['bbox-1']\n",
    "        zmin = row['bbox-2']\n",
    "        xmax = row['bbox-3']\n",
    "        ymax = row['bbox-4']\n",
    "        zmax = row['bbox-5']\n",
    "        gt_region = gt[xmin:xmax, ymin:ymax, zmin:zmax]\n",
    "        gt_region = (gt_region == gt_lesion)\n",
    "        pred_region = pred[xmin:xmax, ymin:ymax, zmin:zmax]\n",
    "        overlaps = overlap_info(pred_region, gt_region, gt_lesion, overlaps, pred_labels)\n",
    "    return overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "def load_nifti_volume(file_path):\n",
    "    nifti_image = nib.load(file_path)\n",
    "    volume = nifti_image.get_fdata()\n",
    "    return volume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = load_nifti_volume('../stacks/svuh/ground_truth/subID129_0.nii.gz')\n",
    "pred = load_nifti_volume('../stacks/svuh/beams_siamese_unet/subID129_0.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "pred volume 109\n",
      "gt volume 278\n",
      "intersection volume 107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: [{'gt_lesion': 1, 'overlap': 0.0}, {'gt_lesion': 2, 'overlap': 0.0}],\n",
       " 2: [{'gt_lesion': 1, 'overlap': 0.0}, {'gt_lesion': 2, 'overlap': 0.0}],\n",
       " 3: [{'gt_lesion': 1, 'overlap': 0.0}, {'gt_lesion': 2, 'overlap': 0.0}],\n",
       " 4: [{'gt_lesion': 1, 'overlap': 0.0}, {'gt_lesion': 2, 'overlap': 0.0}],\n",
       " 5: [{'gt_lesion': 1, 'overlap': 0.0}, {'gt_lesion': 2, 'overlap': 0.0}],\n",
       " 6: [{'gt_lesion': 1, 'overlap': 0.0}, {'gt_lesion': 2, 'overlap': 0.0}],\n",
       " 7: [{'gt_lesion': 1, 'overlap': 0.0}, {'gt_lesion': 2, 'overlap': 0.0}],\n",
       " 8: [{'gt_lesion': 1, 'overlap': 0.0},\n",
       "  {'gt_lesion': 2, 'overlap': 0.9816513761458884}],\n",
       " 9: [{'gt_lesion': 1, 'overlap': 0.0}, {'gt_lesion': 2, 'overlap': 0.0}],\n",
       " 10: [{'gt_lesion': 1, 'overlap': 0.0}, {'gt_lesion': 2, 'overlap': 0.0}],\n",
       " 11: [{'gt_lesion': 1, 'overlap': 0.0}, {'gt_lesion': 2, 'overlap': 0.0}],\n",
       " 12: [{'gt_lesion': 1, 'overlap': 0.0}, {'gt_lesion': 2, 'overlap': 0.0}],\n",
       " 13: [{'gt_lesion': 1, 'overlap': 0.0}, {'gt_lesion': 2, 'overlap': 0.0}],\n",
       " 14: [{'gt_lesion': 1, 'overlap': 0.0}, {'gt_lesion': 2, 'overlap': 0.0}]}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_lb = measure.label(pred, background=0)\n",
    "gt_lb = measure.label(gt, background=0)\n",
    "ov_info = find_overlaps(pred_lb, gt_lb, labeled=True)\n",
    "ov_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_prediction_labels(pred_labels, overlap_dict, max_gt_label):\n",
    "    updated_labels = pred_labels.copy()\n",
    "    last_label = max_gt_label\n",
    "\n",
    "    for pred_label, overlaps in overlap_dict.items():\n",
    "        if pred_label == 0:\n",
    "            continue  # skip background\n",
    "        \n",
    "        # Find the ground truth lesion with the highest overlap\n",
    "        max_overlap = -1\n",
    "        best_gt_lesion = None\n",
    "        for overlap_info in overlaps:\n",
    "            if overlap_info['overlap'] > max_overlap:\n",
    "                max_overlap = overlap_info['overlap']\n",
    "                best_gt_lesion = overlap_info['gt_lesion']\n",
    "        \n",
    "        if max_overlap > 0:\n",
    "            # Update the prediction labels with the best matching ground truth label\n",
    "            updated_labels[pred_labels == pred_label] = best_gt_lesion\n",
    "        else:\n",
    "            # Assign a unique label for zero overlap lesions\n",
    "            last_label += 1\n",
    "            updated_labels[pred_labels == pred_label] = last_label\n",
    "    \n",
    "    return updated_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = np.unique(pred_lb)\n",
    "updated_pred = update_prediction_labels(measure.label(pred_lb, background=0), ov_info, pred_labels[-1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  2, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(updated_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "pred volume 109\n",
      "gt volume 278\n",
      "intersection volume 107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2: [{'gt_lesion': 1, 'overlap': 0.0},\n",
       "  {'gt_lesion': 2, 'overlap': 0.9816513761458884}],\n",
       " 15: [{'gt_lesion': 1, 'overlap': 0.0}, {'gt_lesion': 2, 'overlap': 0.0}],\n",
       " 16: [{'gt_lesion': 1, 'overlap': 0.0}, {'gt_lesion': 2, 'overlap': 0.0}],\n",
       " 17: [{'gt_lesion': 1, 'overlap': 0.0}, {'gt_lesion': 2, 'overlap': 0.0}],\n",
       " 18: [{'gt_lesion': 1, 'overlap': 0.0}, {'gt_lesion': 2, 'overlap': 0.0}],\n",
       " 19: [{'gt_lesion': 1, 'overlap': 0.0}, {'gt_lesion': 2, 'overlap': 0.0}],\n",
       " 20: [{'gt_lesion': 1, 'overlap': 0.0}, {'gt_lesion': 2, 'overlap': 0.0}],\n",
       " 21: [{'gt_lesion': 1, 'overlap': 0.0}, {'gt_lesion': 2, 'overlap': 0.0}],\n",
       " 22: [{'gt_lesion': 1, 'overlap': 0.0}, {'gt_lesion': 2, 'overlap': 0.0}],\n",
       " 23: [{'gt_lesion': 1, 'overlap': 0.0}, {'gt_lesion': 2, 'overlap': 0.0}],\n",
       " 24: [{'gt_lesion': 1, 'overlap': 0.0}, {'gt_lesion': 2, 'overlap': 0.0}],\n",
       " 25: [{'gt_lesion': 1, 'overlap': 0.0}, {'gt_lesion': 2, 'overlap': 0.0}],\n",
       " 26: [{'gt_lesion': 1, 'overlap': 0.0}, {'gt_lesion': 2, 'overlap': 0.0}],\n",
       " 27: [{'gt_lesion': 1, 'overlap': 0.0}, {'gt_lesion': 2, 'overlap': 0.0}]}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ov_info2 = find_overlaps(updated_pred, gt, labeled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true_labels, y_pred_labels):\n",
    "    # Get unique labels in ground truth and prediction\n",
    "    true_labels = np.unique(y_true_labels)\n",
    "    pred_labels = np.unique(y_pred_labels)\n",
    "    # Initialize counts\n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    \n",
    "    # Calculate true positives\n",
    "    for label in true_labels:\n",
    "        if label == 0:\n",
    "            continue\n",
    "        if label in pred_labels:\n",
    "            true_positive += 1\n",
    "        else:\n",
    "            false_negative += 1\n",
    "    \n",
    "    # Calculate false positives\n",
    "    for label in pred_labels:\n",
    "        if label == 0:\n",
    "            continue\n",
    "        if label not in true_labels:\n",
    "            false_positive += 1\n",
    "    \n",
    "    return true_positive, false_positive, false_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_metrics(model, dataset, target_dir, eps=1e-10):\n",
    "    csv = os.path.join(target_dir, dataset, 'ground_truth.csv')\n",
    "    df = pd.read_csv(csv)\n",
    "    results = []\n",
    "    for i, row in df.iterrows():\n",
    "        filename = row['file']\n",
    "        gt = load_nifti_volume(os.path.join(target_dir, dataset, 'ground_truth', filename))\n",
    "        pred = load_nifti_volume(os.path.join(target_dir, dataset, model, filename))\n",
    "        pred_lb = measure.label(pred, background=0)\n",
    "        gt_lb = measure.label(gt, background=0)\n",
    "        ov_info = find_overlaps(pred_lb, gt_lb, labeled=True)\n",
    "        updated_pred = update_prediction_labels(measure.label(pred_lb, background=0), ov_info, pred_lb.max() )\n",
    "        tp, fp, fn = calculate_metrics(gt_lb, updated_pred)\n",
    "        if tp == 0:\n",
    "            if fp == 0 and fn == 0:\n",
    "                print(f'Perfect prediction for {filename}')\n",
    "                dice = 1\n",
    "                precision = 1\n",
    "                recall = 1\n",
    "            else:\n",
    "                dice = 0\n",
    "                precision = 0\n",
    "                recall = 0\n",
    "        else:\n",
    "            dice = 2*tp / (2*tp + fp + fn + eps)\n",
    "            precision = tp / (tp + fp + eps)\n",
    "            recall = tp / (tp + fn + eps)\n",
    "        results.append({'filename': filename, 'dice': dice, 'precision': precision, 'recall': recall,\n",
    "                        'correct': tp, 'false_positive': fp, 'false_negative': fn})\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'dice' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeams_siamese_unet\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvuh\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m beams_su \u001b[38;5;241m=\u001b[39m \u001b[43mfind_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 21\u001b[0m, in \u001b[0;36mfind_metrics\u001b[0;34m(model, dataset, target_dir, eps)\u001b[0m\n\u001b[1;32m     19\u001b[0m     recall \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m     \u001b[43mdice\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     22\u001b[0m     precision \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     23\u001b[0m     recall \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'dice' referenced before assignment"
     ]
    }
   ],
   "source": [
    "target_dir = '../stacks/'\n",
    "model = 'beams_siamese_unet'\n",
    "dataset = 'svuh'\n",
    "beams_su = find_metrics(model, dataset, target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'siamese_unet'\n",
    "su = find_metrics(model, dataset, target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"att_pfpn\"\n",
    "pfpn = find_metrics(model, dataset, target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"beams_att_pfpn\"\n",
    "beams_pfpn = find_metrics(model, dataset, target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"nnUNet\"\n",
    "nnunet = find_metrics(model, dataset, target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"xbound22\"\n",
    "xbound22 = find_metrics(model, dataset, target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"vitseg_r18\"\n",
    "vit = find_metrics(model, dataset, target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"pfpn\"\n",
    "n_pfpn = find_metrics(model, dataset, target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"beams_pfpn\"\n",
    "n_beams_pfpn = find_metrics(model, dataset, target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = '../stacks/'\n",
    "model = \"beams_siamese_unet_bcef05\"\n",
    "dataset = 'svuh'\n",
    "beams_siamese_p = find_metrics(model, dataset, target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = '../stacks/'\n",
    "model = \"vitseg_r50_backbone_enc_dec_2\"\n",
    "dataset = 'svuh'\n",
    "vit_r50 = find_metrics(model, dataset, target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "beams_su.to_csv('beams_su.csv', index=False)\n",
    "su.to_csv('su.csv', index=False)\n",
    "pfpn.to_csv('pfpn.csv', index=False)\n",
    "beams_pfpn.to_csv('beams_pfpn.csv', index=False)\n",
    "nnunet.to_csv('nnunet.csv', index=False)\n",
    "xbound22.to_csv('xbound22.csv', index=False)\n",
    "vit.to_csv('vit.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def summarize(results):\n",
    "    for model, df in results.items():\n",
    "        print(\"-\"*10)\n",
    "        print(model)\n",
    "        print(\"Dice mean: \", df['dice'].mean())\n",
    "        print(\"Precision mean: \", df['precision'].mean())\n",
    "        print(\"Recall mean: \", df['recall'].mean())\n",
    "        print(\"Correct: \", df['correct'].sum())\n",
    "        print(\"False Positive: \", df['false_positive'].sum())\n",
    "        print(\"False Negative: \", df['false_negative'].sum())\n",
    "        print(\"-\"*10)\n",
    "\n",
    "def summarize_df(results):\n",
    "    data = []\n",
    "    for model, df in results.items():\n",
    "        data.append({'model': model, 'dice': df['dice'].mean(), 'precision': df['precision'].mean(), 'recall': df['recall'].mean(),\n",
    "                    'correct': df['correct'].sum(), 'false_positive': df['false_positive'].sum(), 'false_negative': df['false_negative'].sum()})\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "vit\n",
      "Dice mean:  0.44666083440941334\n",
      "Precision mean:  0.4222563650087221\n",
      "Recall mean:  0.5818518518145306\n",
      "Correct:  65\n",
      "False Positive:  158\n",
      "False Negative:  33\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# results = [(\"nnUNet\", nnunet),\n",
    "#            (\"XboundFormer\", xbound22),\n",
    "#            (\"ViT\", vit),\n",
    "#            (\"att pfpn\", pfpn),\n",
    "#            (\"pfpn\", n_pfpn),\n",
    "#            (\"Siamese U-Net\", su),\n",
    "#            (\"BEAMS att PFPN\", beams_pfpn),\n",
    "#            (\"BEAMS PFPN\", n_beams_pfpn),\n",
    "#            (\"BEAMS Siamese U-Net\", beams_su),\n",
    "#            (\"BEAMS Siamese U-Net P\", beams_siamese_p)]\n",
    "\n",
    "results = {'vit': vit_r50}\n",
    "summarize(dict(results))\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = summarize_df(dict(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llrrrrrr}\n",
      " & model & dice & precision & recall & correct & false_positive & false_negative \\\\\n",
      "0 & vit & 0.446661 & 0.422256 & 0.581852 & 65 & 158 & 33 \\\\\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res_df = summarize_df(dict(results))\n",
    "ltx = res_df.style.to_latex()\n",
    "print(ltx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = '../'\n",
    "model = 'siamc-dfl'\n",
    "dataset = 'ablation_stacks'\n",
    "dfl = find_metrics(model, dataset, target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = '../'\n",
    "model = 'siamc-dice'\n",
    "dataset = 'ablation_stacks'\n",
    "dc = find_metrics(model, dataset, target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = '../'\n",
    "model = 'siamc-f05'\n",
    "dataset = 'ablation_stacks'\n",
    "f05 = find_metrics(model, dataset, target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = '../'\n",
    "model = 'siamc-f2'\n",
    "dataset = 'ablation_stacks'\n",
    "f2 = find_metrics(model, dataset, target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "DiceLoss\n",
      "Dice mean:  0.35083611489812083\n",
      "Precision mean:  0.2842400743071825\n",
      "Recall mean:  0.6660493826740203\n",
      "Correct:  76\n",
      "False Positive:  451\n",
      "False Negative:  22\n",
      "----------\n",
      "----------\n",
      "DiceFocalLoss\n",
      "Dice mean:  0.23836665077550126\n",
      "Precision mean:  0.16098570542053717\n",
      "Recall mean:  0.7228395061260641\n",
      "Correct:  81\n",
      "False Positive:  781\n",
      "False Negative:  17\n",
      "----------\n",
      "----------\n",
      "FBetaLoss-2\n",
      "Dice mean:  0.13212098417061058\n",
      "Precision mean:  0.07624578785951742\n",
      "Recall mean:  0.7420987653848333\n",
      "Correct:  85\n",
      "False Positive:  1429\n",
      "False Negative:  13\n",
      "----------\n",
      "----------\n",
      "FBetaLoss-0.5\n",
      "Dice mean:  0.4700853226095146\n",
      "Precision mean:  0.426500843470907\n",
      "Recall mean:  0.6309876542804093\n",
      "Correct:  71\n",
      "False Positive:  185\n",
      "False Negative:  27\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "results = [(\"DiceLoss\", dc),\n",
    "           (\"DiceFocalLoss\", dfl),\n",
    "           (\"FBetaLoss-2\", f2),\n",
    "           (\"FBetaLoss-0.5\", f05),\n",
    "           ]\n",
    "\n",
    "summarize(dict(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llrrrrrr}\n",
      " & model & dice & precision & recall & correct & false_positive & false_negative \\\\\n",
      "0 & DiceLoss & 0.350836 & 0.284240 & 0.666049 & 76 & 451 & 22 \\\\\n",
      "1 & DiceFocalLoss & 0.238367 & 0.160986 & 0.722840 & 81 & 781 & 17 \\\\\n",
      "2 & FBetaLoss-2 & 0.132121 & 0.076246 & 0.742099 & 85 & 1429 & 13 \\\\\n",
      "3 & FBetaLoss-0.5 & 0.470085 & 0.426501 & 0.630988 & 71 & 185 & 27 \\\\\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res_df = summarize_df(dict(results))\n",
    "ltx = res_df.style.to_latex()\n",
    "print(ltx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = '../stacks/'\n",
    "model = 'nnUNet'\n",
    "dataset = 'ms2'\n",
    "ms2nnunet = find_metrics(model, dataset, target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = '../stacks/'\n",
    "model = 'beams_siamese_unet'\n",
    "dataset = 'ms2'\n",
    "ms2_beams_siamese = find_metrics(model, dataset, target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = '../stacks/'\n",
    "model = 'pfpn'\n",
    "dataset = 'ms2'\n",
    "ms2_pfpn = find_metrics(model, dataset, target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = '../stacks/'\n",
    "model = 'beams_pfpn'\n",
    "dataset = 'ms2'\n",
    "ms2_beams_pfpn = find_metrics(model, dataset, target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = '../stacks/'\n",
    "model = 'siamese_unet'\n",
    "dataset = 'ms2'\n",
    "ms2_siamese = find_metrics(model, dataset, target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = '../stacks/'\n",
    "model = 'xbound22'\n",
    "dataset = 'ms2'\n",
    "ms2_xbound = find_metrics(model, dataset, target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = '../stacks/'\n",
    "model = 'att_pfpn'\n",
    "dataset = 'ms2'\n",
    "ms2_att_pfpn = find_metrics(model, dataset, target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = '../stacks/'\n",
    "model = 'beams_att_pfpn'\n",
    "dataset = 'ms2'\n",
    "ms2_beams_att_pfpn = find_metrics(model, dataset, target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = '../stacks/'\n",
    "model = \"beams_siamese_unet_f05p\"\n",
    "dataset = 'ms2'\n",
    "ms2_beams_siamese_f05p = find_metrics(model, dataset, target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = '../stacks/'\n",
    "model = \"beams_siamese_unet_no_p\"\n",
    "dataset = 'ms2'\n",
    "ms2_beams_siamese_nop = find_metrics(model, dataset, target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = '../stacks/'\n",
    "model = \"beams_siamese_unet_bcef05\"\n",
    "dataset = 'ms2'\n",
    "ms2_beams_siamese_p = find_metrics(model, dataset, target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = '../stacks/'\n",
    "model = \"vitseg_r50_backbone_enc_dec_2\"\n",
    "dataset = 'ms2'\n",
    "ms2_vit_r50 = find_metrics(model, dataset, target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms2_xbound.to_csv('ms2_xbound.csv', index=False)\n",
    "ms2_siamese.to_csv('ms2_siamese.csv', index=False)\n",
    "ms2_beams_siamese.to_csv('ms2_beams_siamese.csv', index=False)\n",
    "ms2_pfpn.to_csv('ms2_pfpn.csv', index=False)\n",
    "ms2_beams_pfpn.to_csv('ms2_beams_pfpn.csv', index=False)\n",
    "ms2nnunet.to_csv('ms2_nnunet.csv', index=False)\n",
    "ms2_att_pfpn.to_csv('ms2_att_pfpn.csv', index=False)\n",
    "ms2_beams_att_pfpn.to_csv('ms2_beams_att_pfpn.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "vit\n",
      "Dice mean:  0.2210874140447432\n",
      "Precision mean:  0.34435515871170735\n",
      "Recall mean:  0.21779801291145096\n",
      "Correct:  50\n",
      "False Positive:  51\n",
      "False Negative:  95\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# results = [(\"nnUNet\", ms2nnunet),\n",
    "#            (\"XboundFormer\", ms2_xbound),\n",
    "#            (\"pfpn\", ms2_pfpn),\n",
    "#            (\"Siamese U-Net\", ms2_siamese),\n",
    "           \n",
    "#            (\"BEAMS Siamese U-Net P\", ms2_beams_siamese_p),\n",
    "#            (\"BEAMS PFPN\", ms2_beams_pfpn),\n",
    "#            ]\n",
    "results = {'vit': ms2_vit_r50}\n",
    "\n",
    "\n",
    "summarize(dict(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llrrrrrr}\n",
      " & model & dice & precision & recall & correct & false_positive & false_negative \\\\\n",
      "0 & vit & 0.221087 & 0.344355 & 0.217798 & 50 & 51 & 95 \\\\\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res_df = summarize_df(dict(results))\n",
    "ltx = res_df.style.to_latex()\n",
    "print(ltx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glasses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
