import sys
import os

# Get the current directory
current_dir = os.getcwd()

# Get the parent directory
parent_dir = os.path.dirname(current_dir)

# Add the parent directory to the Python path
sys.path.append(parent_dir)

import argparse
import torch
from torch.utils.tensorboard import SummaryWriter
import torch.nn.functional as F
from boundary_aware.siamese_unet import SiameseNetwork
from boundary_aware.transformer import BAViTSeg
from boundary_aware.unet import BAUNet
from utils.utils import get_loss_function
from dataset import get_siamese_train_loaders
from train.training import train_cd_ba_model, validate_cd_ba_model


def check_keys(model, pretrain_path):
    # Check for matching keys
    pretrain = torch.load(pretrain_path)
    pretrained_keys = set(pretrain.keys())
    model_keys = set(model.state_dict().keys())

    missing_keys = model_keys - pretrained_keys
    unexpected_keys = pretrained_keys - model_keys

    if len(missing_keys) > 0:
        print("Missing keys in Siamese network:", missing_keys)
    else: 
        print("No Missing keys")

    if len(unexpected_keys) > 0:
        print("Unexpected keys in pre-trained UNet:", unexpected_keys)
    else:
        print("No unexpected keys")


def parse_args():
    parser = argparse.ArgumentParser(description='Train baselines')
    parser.add_argument('--model_name', type=str, default='unet', help='Model name to use')
    parser.add_argument('--epochs', type=int, default=50, help='Number of epochs to train')
    parser.add_argument('--learning_rate', type=float, default=2e-5, help='Learning rate')
    parser.add_argument('--optimizer', type=str, default='Adam', help='Optimizer to use')
    parser.add_argument('--pretrain', action='store_true', help='Use pre-trained model')
    parser.add_argument('--pretrain_path', type=str, default="", help='Path to pre-trained model')
    parser.add_argument('--mask_loss', type=str, choices=['f0.5', 'f1', 'f2'], default='f0.5', help='Mask loss function')
    parser.add_argument('--boundary_loss', type=str, choices=['f0.5', 'f1', 'f2','bce'], default='bce', help='Boundary loss function')
    parser.add_argument('--data_directory', type=str, default='', help='Path to data directory')
    parser.add_argument('--batch_size', type=int, default=12, help='Batch size')
    parser.add_argument('--training_file', type=str, default='train.csv', help='Training file')
    parser.add_argument('--validation_file', type=str, default='valid.csv', help='Validation file')
    parser.add_argument('--gpu', type=str, default='cuda:0', help='GPU to use')
    return parser.parse_args()

if __name__ == '__main__':
    args = parse_args()
    print("\n\n\n\n\n\n")
    print("*"*100)
    print(f"Training {args.model_name} with mask loss {args.mask_loss} and boundary loss {args.boundary_loss}")
    print("*"*100)
    print("\n\n\n\n\n\n")

    learning_rate = args.learning_rate
    loss_mask = get_loss_function(args.mask_loss)
    loss_boundary = get_loss_function(args.boundary_loss)
    epochs = args.epochs
    pretrain = args.pretrain
    pretrain_path = args.pretrain_path
    data_directory = args.data_directory
    train_file = args.training_file
    valid_file = args.validation_file
    batch_size = args.batch_size
    data_directory = args.data_directory
    device = args.gpu if torch.cuda.is_available() else 'cpu'
    
    # Initialize TensorBoard
    writer = SummaryWriter()

    if args.model_name == 'SiameseNetwork':
        model = SiameseNetwork(in_channels=1, out_channels=1)
    if args.model_name == "transformer":
        model = BAViTSeg(in_channels=1, num_classes=1, with_pos='learned')
    else:
        raise ValueError("Unsupported model name")

    if pretrain:
        check_keys(model, pretrain_path)
        model.load_state_dict(torch.load(pretrain_path), strict=False)
    
    train_dataloader, valid_dataloader = get_siamese_train_loaders(train_file, valid_file,
                                                                   data_directory,batch_size, with_boundary=True)
    
    if args.optimizer == 'Adam':
        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    elif args.optimizer == 'SGD':
        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
    else:
        raise ValueError("Unsupported optimizer")

    ckpt=f"{args.model_name}_{args.mask_loss}_{args.boundary_loss}_epochs{epochs}.pth"
    
    # lists to keep track of losses and accuracies
    best_loss = 0
    best_epoch = 0

    # start the training
    model = model.to(device)

    for epoch in range(epochs):
        print(f"[INFO]: Epoch {epoch+1} of {epochs}")
        train_epoch_losses, train_epoch_acc, train_epoch_dice, train_epoch_f1 = train_cd_ba_model(model, train_dataloader,
                                                optimizer, loss_mask, loss_boundary ,device)
        valid_epoch_losses, valid_epoch_acc, valid_epoch_dice, valid_epoch_f1 = validate_cd_ba_model(model, valid_dataloader,
                                                    loss_mask, loss_boundary, device)

        valid_loss = valid_epoch_losses[0]

        writer.add_scalar('Train/Loss', train_epoch_losses[0], epoch)
        writer.add_scalar('Train/Accuracy', train_epoch_acc, epoch)
        writer.add_scalar('Train/Dice', train_epoch_dice, epoch)
        writer.add_scalar('Train/F1', train_epoch_f1, epoch)
        writer.add_scalar('Validation/Loss', valid_epoch_losses[0], epoch)
        writer.add_scalar('Validation/Accuracy', valid_epoch_acc, epoch)
        writer.add_scalar('Validation/Dice', valid_epoch_dice, epoch)
        writer.add_scalar('Validation/F1', valid_epoch_f1, epoch)

        print(f"Training : {train_epoch_losses[0]:.3f}, b: {train_epoch_losses[2]:.3f}, m: {train_epoch_losses[1]:.3f}, training acc: {train_epoch_acc:.3f}, dice : {train_epoch_dice:.3f}, f1: {train_epoch_f1:.3f}")
        print(f"Validation : {valid_epoch_losses[0]:.3f}, b: {valid_epoch_losses[2]:.3f},m: {valid_epoch_losses[1]:.3f}, validation acc: {valid_epoch_acc:.3f}, dice : {valid_epoch_dice:.3f}, f1: {valid_epoch_f1:.3f}")

        if epoch==0:
            print("saving first model")
            best_loss=valid_epoch_losses[0]
            best_epoch=epoch
            torch.save(model.state_dict(), ckpt)
        elif valid_epoch_losses[0] < best_loss:
            print(f"model loss improved from {best_loss} to {valid_epoch_losses[0]} : saving best")
            best_loss=valid_epoch_losses[0]
            best_epoch=epoch
            torch.save(model.state_dict(), ckpt)
        else:
            print(f"val loss did not improve from {best_loss} @ {best_epoch+1}")

        print('-'*100)
    writer.close()
